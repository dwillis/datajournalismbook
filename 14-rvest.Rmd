# Scraping data with Rvest

Sometimes, governments put data online on a page or in a searchable database. And when you ask them for a copy of the data underneath the website, they say no. Why? Because they have a website. That's it. That's their reason. We don't have to give you the data because we've already given you the data, never mind that they haven't given to you in a form you can actually load into R with ease.

Lucky for us, there's a way for us to write code to get data even when an agency hasn't made it easy: webscraping.

One of the most powerful tools you can learn as a data journalist is how to scrape data from the web. Scraping is the process of programming a computer to act like a browser, go to a website, injest the HTML from that website and turn it into data. 

The degree of difficulty here goes from Easy to So Hard You Want To Throw Your Laptop Out A Window. And the curve between the two steep. You can learn how to scrape Easy in a day. The hard ones take a little more time.  In this chapter, we'll show you an easy one.  And in the next chapter, we'll so you a harder one.

Let's start easy. 

We're going to use a library called `rvest`, which you can install it the same way we've done all installs: go to the console and `install.packages("rvest")`. 

We'll load these packages first:

```{r}
library(rvest)
library(tidyverse)
library(janitor)
```

For this example, we're going to work on loading a simple table of data from the Bureau of Labor Statistics, a table of industry sectors by NAICS code we could make use of in our analysis of PPP loan data. 

Recall that our PPP loan data has six-digit NAICS codes for each industry, which allows us to identify the industry for each loan.  For example 212221 is the code for "Gold Mining Industry". 

A six-digit NAICS code is the most specific.  As we remove numbers from the right to create five-digit, four-digit, three-digit and two-digit codes, the industries they represent get broader. Here's an example:

* 212221 - Gold Mining Industry
* 2122 - Metal Ore Mining Industry (which also contains things like silver mining, iron mining and copper mining)
* 21 - Mining, Quarrying, and Oil and Gas Extraction Industry (which also contains mining industry mentioned above, but also oil drilling, coal mining and others)

It might be useful to have a lookup table of those top-level, two-digit NAICS codes (also called sector codes) for our analysis, to help us answer questions about what specific top-level industries got the highest loan amounts.  Let's suppose we can't find a table like that for download, but we do see a version on the BLS website at this URL: [https://www.bls.gov/ces/naics/](https://www.bls.gov/ces/naics/).

```{r, echo=FALSE}
knitr::include_graphics(rep("images/rvest1.png"))
```
We could get this table into R with the following manual steps: highlighting the text, copying it into Excel, saving it as a csv, and reading it into R.  Or, we could write a few lines of webscraping code to have R do that for us! 

In this simple example, it's probably faster to do it manually than have R do it for us. And this table is unlikely to change much in the future. Why would we ever write code to grab a single table? There's several reasons:

* Our methods are transparent.  If someone wants to run our code from scratch, they don't need to repeat the manual steps, which may not be documented very well. 
* Let's suppose we wanted to grab the same table every day, to monitor for changes (like, say, updated COVID case numbers).  Writing a script once, and pressing a single button every day is going to be much more efficient than doing this manually every day. 
* If we're doing it manually, we're more likely to make a mistake, like maybe failing to copy the whole table.
* As we'll see in the next chapter, if we ever want to grab the same table from multiple pages, writing code is much faster and easier than doing a bunch of manual steps. 

So, to scrape, the first thing we need to do is start with the URL. Let's store it as an object called naics_url.

```{r}
naics_url <- "https://www.bls.gov/ces/naics/"
```

When we go to the web page, we can see a nicely-designed page that contains our information.  But what we really care about, for our purposes, is the html code that creates that page.  

In our web browser, if we right-click anywhere on the page and select "view source" from the popup menu, we can see the source code.  Here's a picture of what some of it looks like. 

```{r, echo=FALSE}
knitr::include_graphics(rep("images/rvest2.png"))
```
We'll use those HTML tags -- things like <div> and <a> and <table> -- to grab the info we need. 

Okay, step 1.  Let's write a bit of code to tell R to go to the URL and ingest all of that HTML code. In the code below, we're starting with our URL and using the read_html() function from rvest to ingest all of the page html, storing it as an object called naics_industry. 

```{r}

naics_industry <- naics_url %>%
  read_html() 

```

In our environment window, you'll see naics_industry as a "list of 2".  This is not a dataframe, it's a different type of data structure a "nested list". 

If we click on naics_industry in our environment window, we can see that it's pulled in the html. Nested within the <html> tag is the <head> and <body>, the two fundamental sections of most web pages.  We're going to pull information out of the <body> tag. 

```{r, echo=FALSE}
knitr::include_graphics(rep("images/rvest3.png"))
```
Now, our task is to just pull out the section of the html that contains the information we need.  

But which part? To figure that out, we can go back to the page in a web browser like chrome, and use built in tools to "inspect" the html code underlying the page.  

Let's find the data we want to grab -- "Table 2. NAICS Sectors" - and right click on the word "Sector" in the column header of the table.  That will bring up a dropdown menu. Select "inspect", which will pop up a window called the "element inspector".   

```{r, echo=FALSE}
knitr::include_graphics(rep("images/rvest4.png"))
```

Note that the entire table is actually contained inside an html <table>, which has a header row <thead> that contains the column names and a <tbody> that contains one row <tr> per industry sector code. 

Because it's inside of a table, and not some other kind of element (like a <div>), rvest has a special function for easily extracting and converting html tables, called html_table().  

```{r}

naics_industry  <- naics_url %>%
  read_html() %>%
  html_table()

```

In the environment window, look at naics_industry.  Note that it's now a "list of 6".  
Click on it to open it up.  It should look like this. 

```{r, echo=FALSE}
knitr::include_graphics(rep("images/rvest5.png"))
```

This gets a little complicated, but what you're seeing here is a nested list that contains six different data frames -- also called tibbles --  one for each table that exists on the web page we scraped. They're numbered 1 to 6.  The first 1 has 4 rows and 3 columns, the second has 21 rows and 2 columns, and so on.   

To examine what's in each dataframe, mouse over the right edge (next to the word columns) on each row, and click the little scroll icon.  The icon will be hidden until you mouse over it.  

Let's click on the scroll icon for the first dataframe examine it.  

```{r, echo=FALSE}
knitr::include_graphics(rep("images/rvest6.png"))
```
That's not the one we want!  

Let's try clicking on the scroll icon for row 2. 

```{r, echo=FALSE}
knitr::include_graphics(rep("images/rvest7.png"))
```

That's more like it! So, all we need to do now is to store that single dataframe as an object, and get rid of the rest.  We can do that with this code, which says "keep only the second dataframe from our nested list. 

```{r}
# Read in all html from table, store all tables on page as nested list of dataframes.
naics_industry  <- naics_url %>%
  read_html() %>%
  html_table()

# Just keep the second dataframe in our list

naics_industry <- naics_industry[[2]]

# show the dataframe

naics_industry

```

From here, we can do a little light cleaning. Let's use clean_names() to standardize the column names.  Then let's use slice() to remove the last row, which contains source information that will complicate our use of this table later. 

```{r}
# Read in all html from table, store all tables on page as nested list of dataframes.
naics_industry  <- url %>%
  read_html() %>%
  html_table()

# Just keep the second dataframe in our list, standardize column headers, remove last row

naics_industry <- naics_industry[[2]] %>%
  clean_names() %>%
  slice(-21)

# show the dataframe
naics_industry

```

And there we go. A nice tidy dataframe of NAICS sector codes.  

In the next chapter, we'll look at a more complicated example. 


