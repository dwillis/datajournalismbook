# Data Cleaning Part I: Data smells

Any time you are given a dataset from anyone, you should immediately be suspicious. Is this data what I think it is? Does it include what I expect? Is there anything I need to know about it? Will it produce the information I expect?

One of the first things you should do is give it the smell test. 

Failure to give data the smell test [can lead you to miss stories and get your butt kicked on a competitive story](https://source.opennews.org/en-US/learning/handling-data-about-race-and-ethnicity/).

Let's look at some campus parking ticket data. You can [get it here](https://unl.box.com/s/3c5kx2i5iouc52ty46k4js412u48yajr).

With data smells, we're trying to find common mistakes in data. [For more on data smells, read the GitHub wiki post that started it all](https://github.com/nikeiubel/data-smells/wiki/Ensuring-Accuracy-in-Data-Journalism). The common mistakes we're looking for are:,

* Missing data
* Gaps in data
* Wrong type of data
* Outliers
* Sharp curves
* Conflicting information within a dataset
* Conflicting information across datasets
* Wrongly derived data
* Internal inconsistency
* External inconsistency
* Wrong spatial data
* Unusable data, including non-standard abbreviations, ambiguous data, extraneous data, inconsistent data

Not all of these data smells are detectable in code. You may have to ask people about the data. You may have to compare it to another dataset yourself. Does the agency that uses the data produce reports from the data? Does your analysis match those reports? That will expose wrongly derived data, or wrong units, or mistakes you made with inclusion or exclusion.

But with several of these data smells, we can do them first, before we do anything else. 

## Wrong Type

First, let's look at **Wrong Type Of Data**. 

We can sniff that out by looking at the output of `readr`. 

Let's load the Maryland slice of ppp_loan_data we've used previously.

```{r}
library(tidyverse)
```

```{r}
ppp_loans_md <- read_csv("data/ppp_loan_data/processed/md/ppp_loans_md.csv.gz")
x<-read_rds("data/ppp_loan_data/processed/md/ppp_loans_md.rmd")

```
Examine the data output message. There are "12802 parsing failures."

Why? Look closely. The `readr` library makes guesses on what data type to assign to each column based on the first 1,000 rows of data.  In the column "old_slug", there are no values for the first 1,000 rows.  They're all "NA".  `readr` just guesses what data type to assign it, so it chooses "logical", where the values it expects will be TRUE or FALSE, T or F, 1 or 0.  Then, on row 170,056, actual values start showing up in that "old_slug" column.  And those values look nothing like "TRUE or FALSE"; they're a long string of numbers and letters like "aditi-llc-d139eb25916444a03984f38eebda4e63", which should be stored as a "character".  Because `readr` has already set this as a logical column, though, it fails to parse correctly.  So it reads in as "NA", even though there's data to work with. 

The easy way to fix this is to change the guess_max parameter of `readr` to use every row to determine what data type to make each column to guess the column types. It'll go a little slower, but it'll fix the problem.  This data set has 195,685 rows, so that's the value we'll use. 

```{r}
ppp_loans_md <- read_csv("data/ppp_loan_data/processed/md/ppp_loans_md.csv.gz", guess_max=195865)
```
This time, we got no parsing failures.  And if we examine the data types `readr` assigned to each column, they generally make sense. Things that should be characters -- like state, city, name -- are characters. Things that should be numbers (dbl) -- like amount -- are numbers.  There are some minor problems that don't warrant fixing, necessarily.  The id column is a good example.  It read in as a number (dbl), which makes sense, because it really is just a string of numbers.  But we'd never need to do math on these values; it wouldn't make sense to add two ids together, for example.  So it is probably best stored as a character. The opposite would be more problematic.  If something that should be stored as a number we want to do math on was stored as a character, we couldn't actually use it to do math. 

```{r}
glimpse(ppp_loans_md)
```
For this, things seem to be good. `Date` appears to be in date format, things that aren't numbers appear to be text. That's a good start.

## Missing Data

The second smell we can find in code is **missing data**. 

We can do that by grouping and counting columns. In addition to identifying the presence of NA values, this method will also give us a sense of the distribution of values in those columns. 

Let's start with the "franchise" column. The following code groups by the franchise name column, counts the number in each group, and then sorts from highest to lowest.   

There are 192,959 NA values in this column.  This makes sense. Not every business will be a franchisee of a larger company like Subway or Dunkin'. In this case, the presence of so many NAs isn't really concerning. 

```{r}

ppp_loans_md %>% 
  group_by(franchise_name) %>% 
  summarise(
    count=n()
  ) %>%
  arrange(desc(count))
```
Now let's try the "forgiveness_amount" column, which represents the amount of the loan that was forgiven, or not required to be paid back. In this case, there are 135,073 NA values. The rest have different dollar amounts.  

```{r}

ppp_loans_md %>% 
  group_by(forgiveness_amount) %>% 
  summarise(
    count=n()
  ) %>%
  arrange(desc(count))
```
Do the 135,073 NAs represent loans that weren't forgiven? Do they represent loans that might have been forgiven, but the data is simply missing the amount of money?  

We could check the documentation, which isn't particularly helpful.  It only says the column represents "forgiveness amount." 

We could check the "forgiveness_date" column, to see how many NAs it has: 135,703, the same number as the number of NAs in "forgiveness_amount".   

```{r}

ppp_loans_md %>% 
  group_by(forgiveness_date) %>% 
  summarise(
    count=n()
  ) %>%
  filter(is.na(forgiveness_date)) %>%
  arrange(desc(count))
```

We can group by forgiveness_amount and forgiveness_date to determine whether one is NA when the other is NA.  Because this grouping has 135,073 rows, too, we know this to be the case. 

```{r}
ppp_loans_md %>%
  group_by(forgiveness_amount, forgiveness_date) %>%
  summarise(
    count=n()
  ) %>%
  #filter(is.na(forgiveness_date)) %>%
  arrange(desc(count))

```

Before we decide to base a publishable finding on this column, we should call the custodian of the data to confirm our interpretation of NA values. 

## Gaps in data

Let's now look at **gaps in data**. It's been my experience that gaps in data often have to do with time, so let's first look at "date_approved", so we can see if there's any missing months, or huge differences in the number of loans by month. Let's start with Date. If we're going to work with dates, we should have `lubridate` handy for `floor_date`. The `floor_date` function will allow us to group by month, instead of a single day. 

```{r}
library(lubridate)
```


```{r}
ppp_loans_md %>% 
  mutate(month_year_approved = floor_date(date_approved, "month")) %>%
  group_by(month_year_approved) %>% 
   summarise(
    count=n()
  ) %>%
  arrange(month_year_approved)
```

So, our data starts in April 2020, the month that has more loans -- 45,040 -- than any other month.  That makes sense, as the program launched at the start of the pandemic, in April 2020.  The number of loans declines each month through August 2020, as the initial round of the program worked through the initial funding allocation.  

Then, there are no loans until January 2021. EXPLAIN WHY? IS THIS A PROBLEM?  There are loans every month until the program winds down on May 31, 2021.  There are still loans in June.  IS THIS A PROBLEM.   

## Internal inconsistency

Any time you are going to focus on something, you should check it for consistency inside the data set. So let's pretend the large number of Displaying Altered Permit tickets caught your attention and you want to do a story about tens of thousands of students being caught altering their parking permits to reduce their costs parking on campus. Good story right? Before you go calling the parking office for comment, I'd check that data first. 

```{r}
tickets %>% filter(Violation == "Displaying Altered Permit") %>% group_by(floor_date(Date, "month")) %>% tally()
```

So this charge exists when our data starts, but scroll forward: In October 2013, there's 1,081 tickets written. A month later, only 121. A month after that? 1. And then one sporadically for three more years. 

Something major changed. What is it? That's why you are a reporter. Go ask. But we know our data doesn't support the story we started with.

And that's what Data Smells are designed to do: stop you from going down a bad path.

## A Shortcut: Summary

One quick way to get some data smells is to use R's built in summary function. What summary does is run summary statistics on each column of your dataset. Some of the output is ... underwhelming ... but some is really useful. For example,  looking at min and max for dates can point to bad data there. Min and max will also show you out of range numbers -- numbers far too big or small to make sense. 

The syntax is simple. 

```{r}
summary(tickets)
```

In this case, the output doesn't do much for us except dates. Looking at the min and max will tell us if we have any out of range dates. In this case, we do not. 
