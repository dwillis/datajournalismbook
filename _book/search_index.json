[
["data-cleaning-part-i-data-smells.html", "Chapter 10 Data Cleaning Part I: Data smells 10.1 Wrong Type 10.2 Missing Data 10.3 Gaps in data 10.4 Internal inconsistency", " Chapter 10 Data Cleaning Part I: Data smells Any time you are given a dataset from anyone, you should immediately be suspicious. Is this data what I think it is? Does it include what I expect? Is there anything I need to know about it? Will it produce the information I expect? One of the first things you should do is give it the smell test. Failure to give data the smell test can lead you to miss stories and get your butt kicked on a competitive story. Let’s look at some campus parking ticket data. You can get it here. With data smells, we’re trying to find common mistakes in data. For more on data smells, read the GitHub wiki post that started it all. The common mistakes we’re looking for are:, Missing data Gaps in data Wrong type of data Outliers Sharp curves Conflicting information within a dataset Conflicting information across datasets Wrongly derived data Internal inconsistency External inconsistency Wrong spatial data Unusuable data, including non-standard abbreviations, ambigious data, extraneous data, inconsistent data Not all of these data smells are detectable in code. You may have to ask people about the data. You may have to compare it to another dataset yourself. Does the agency that uses the data produce reports from the data? Does your analysis match those reports? That will expose wrongly derived data, or wrong units, or mistakes you made with inclusion or exclusion. But with several of these data smells, we can do them first, before we do anything else. 10.1 Wrong Type First, let’s look at Wrong Type Of Data. We can sniff that out by looking at the output of readr library(tidyverse) ## Warning: package &#39;tidyverse&#39; was built under R version 3.5.2 ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.2.1 ✓ purrr 0.3.3 ## ✓ tibble 2.1.3 ✓ dplyr 0.8.3 ## ✓ tidyr 1.0.0 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.4.0 ## Warning: package &#39;ggplot2&#39; was built under R version 3.5.2 ## Warning: package &#39;tibble&#39; was built under R version 3.5.2 ## Warning: package &#39;tidyr&#39; was built under R version 3.5.2 ## Warning: package &#39;purrr&#39; was built under R version 3.5.2 ## Warning: package &#39;dplyr&#39; was built under R version 3.5.2 ## Warning: package &#39;stringr&#39; was built under R version 3.5.2 ## Warning: package &#39;forcats&#39; was built under R version 3.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() tickets &lt;- read_csv(&quot;data/tickets.csv&quot;) ## Parsed with column specification: ## cols( ## Citation = col_double(), ## Date = col_datetime(format = &quot;&quot;), ## Location = col_character(), ## Violation = col_character() ## ) ## Warning: 104265 parsing failures. ## row col expected actual file ## 56735 Citation a double T2TEST &#39;data/tickets.csv&#39; ## 57050 Citation a double EF0800001 &#39;data/tickets.csv&#39; ## 57051 Citation a double EF0300004 &#39;data/tickets.csv&#39; ## 57052 Citation a double EF0300005 &#39;data/tickets.csv&#39; ## 57053 Citation a double EF010020 &#39;data/tickets.csv&#39; ## ..... ........ ........ ......... .................. ## See problems(...) for more details. Right away, we see there’s 104,265 parsing errors. Why? Look closely. The Citation number that readr interprets from the first rows comes in at a number. But 56,000 rows in, those citation numbers start having letters in them, and letters are not numbers. The cheap way to fix this is to change the guess_max parameter of readr to just use more than a few rows to guess the column types. It’ll go a little slower, but it’ll fix the problem. tickets &lt;- read_csv(&quot;data/tickets.csv&quot;, guess_max = 60000) ## Parsed with column specification: ## cols( ## Citation = col_character(), ## Date = col_datetime(format = &quot;&quot;), ## Location = col_character(), ## Violation = col_character() ## ) For this, things seem to be good. Date appears to be in date format, things that aren’t numbers appear to be text. That’s a good start. 10.2 Missing Data The second smell we can find in code is missing data. We can do that through a series of Group By and Count steps. tickets %&gt;% group_by(Location) %&gt;% tally() ## # A tibble: 247 x 2 ## Location n ## &lt;chr&gt; &lt;int&gt; ## 1 1001 Y Street 508 ## 2 10th &amp; Q Street 303 ## 3 10th &amp; U Street 222 ## 4 1101 Y Street 83 ## 5 11th &amp; Y Street 38 ## 6 1235 Military Road 33 ## 7 1320 Q 1 ## 8 13th &amp; R Lot 4918 ## 9 14th &amp; Avery Lot 1601 ## 10 14th &amp; Avery Parking Garage 2494 ## # … with 237 more rows What we’re looking for here are blanks: Tickets without a location. Typically, those will appear first or last, depending on several things, so it’s worth checking the front and back of our data. What about ticket type? tickets %&gt;% group_by(Violation) %&gt;% tally() ## # A tibble: 25 x 2 ## Violation n ## &lt;chr&gt; &lt;int&gt; ## 1 Damage Property 25 ## 2 Displaying Altered Permit 23280 ## 3 Displaying Counterfeit Permit 18 ## 4 Displaying Stolen Permit 4 ## 5 Expired Meter 45072 ## 6 Failure to Pay[on exit] 251 ## 7 Failure to Reg. Veh to Permit 53 ## 8 Failure to Register Veh w/ UNL 113 ## 9 False Lost/Stolen Permit Rept 927 ## 10 Falsify Permit Application 3 ## # … with 15 more rows None here either, so that’s good. It means our tickets will always have a location and a violation type. 10.3 Gaps in data Let’s now look at gaps in data. It’s been my experience that gaps in data often have to do with time, so let’s first look at ticket dates, so we can see if there’s any big jumps in data. You’d expect the numbers to change, but not by huge amounts. Huge change would indicate, more often than not, that the data is missing. Let’s start with Date. If we’re going to work with dates, we should have lubridate handy for floor_date. library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following object is masked from &#39;package:base&#39;: ## ## date tickets %&gt;% group_by(floor_date(Date, &quot;month&quot;)) %&gt;% tally() ## # A tibble: 56 x 2 ## `floor_date(Date, &quot;month&quot;)` n ## &lt;dttm&gt; &lt;int&gt; ## 1 2012-04-01 00:00:00 3473 ## 2 2012-05-01 00:00:00 2572 ## 3 2012-06-01 00:00:00 2478 ## 4 2012-07-01 00:00:00 2134 ## 5 2012-08-01 00:00:00 3774 ## 6 2012-09-01 00:00:00 4138 ## 7 2012-10-01 00:00:00 4173 ## 8 2012-11-01 00:00:00 3504 ## 9 2012-12-01 00:00:00 1593 ## 10 2013-01-01 00:00:00 3078 ## # … with 46 more rows First thing to notice: our data starts in April 2012. So 2012 isn’t a complete year. Then, scroll through. Look at December 2013 - March 2014. The number of tickets drops to about 10 percent of normal. That’s … odd. And then let’s look at the end – November 2016. So not a complete year in 2016 either. 10.4 Internal inconsistency Any time you are going to focus on something, you should check it for consistency inside the data set. So let’s pretend the large number of Displaying Altered Permit tickets caught your attention and you want to do a story about tens of thousands of students being caught altering their parking permits to reduce their costs parking on campus. Good story right? Before you go calling the parking office for comment, I’d check that data first. tickets %&gt;% filter(Violation == &quot;Displaying Altered Permit&quot;) %&gt;% group_by(floor_date(Date, &quot;month&quot;)) %&gt;% tally() ## # A tibble: 29 x 2 ## `floor_date(Date, &quot;month&quot;)` n ## &lt;dttm&gt; &lt;int&gt; ## 1 2012-04-01 00:00:00 1072 ## 2 2012-05-01 00:00:00 1283 ## 3 2012-06-01 00:00:00 1324 ## 4 2012-07-01 00:00:00 1357 ## 5 2012-08-01 00:00:00 2249 ## 6 2012-09-01 00:00:00 1797 ## 7 2012-10-01 00:00:00 1588 ## 8 2012-11-01 00:00:00 1183 ## 9 2012-12-01 00:00:00 458 ## 10 2013-01-01 00:00:00 1132 ## # … with 19 more rows So this charge exists when our data starts, but scroll forward: In October 2013, there’s 1,081 tickets written. A month later, only 121. A month after that? 1. And then one sporadically for three more years. Something major changed. What is it? That’s why you are a reporter. Go ask. But we know our data doesn’t support the story we started with. And that’s what Data Smells are designed to do: stop you from going down a bad path. "]
]
