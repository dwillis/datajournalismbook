---
title: "lab_06"
author: "derek willis"
date: "8/25/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## You will need

* Tabula

## Load libraries and establish settings
```{r}
# Turn off scientific notation
options(scipen=999)

# Load the tidyverse.
library(tidyverse)
```


```{r}

library(rvest)
library(tidyverse)
library(janitor)
```

```{r}
doj_url <- "https://www.justice.gov/criminal-fraud/cares-act-fraud"

# Get employment html
cases <- doj_url %>%
  read_html()  %>%
  html_elements('ul li b') %>%
  html_text() %>%
  as_tibble() %>%
  rename(cases = value) %>%
  mutate(cases = str_remove_all(cases,"U.S. v|\\.")) %>%
  mutate(cases = str_trim(cases,side="both"))
  

# Display it so we can see what it looks like
cases
```

```{r}
doj_url <- "https://www.justice.gov/criminal-fraud/cares-act-fraud"

# Get employment html
cases <- doj_url %>%
  read_html()  %>%
  html_elements('p') %>%
  html_text() %>%
  as_tibble() %>%
  slice(14:41) %>%
  rename(districts = value) %>%
  mutate(districts = str_trim(districts,side="both")) %>%
  mutate(cares_act_cases = "yes")
  

# Display it so we can see what it looks like
cases
```

```{r}
wiki_url <- "https://en.wikipedia.org/wiki/List_of_United_States_district_and_territorial_courts"

# Get employment html
courts <- wiki_url %>%
  read_html()  %>%
  html_table() 

# 
courts <- courts[[3]] %>%
  clean_names()
  
x <- courts %>%
  left_join(cases, by=c("region"="districts")) %>%
  mutate(cares_act_cases = case_when(
    is.na(cares_act_cases) ~ "no",
    TRUE ~ cares_act_cases
  )) %>%
  group_by(cares_act_cases) %>%
  count()


# Display it so we can see what it looks like
x
```
Examine charging documents
Describe

```{r}

# Define API Key

library(tidycensus)
census_api_key("549950d36c22ff16455fe196bbbd01d63cfbe6cf")
population_state<- get_acs(geography="state", variables=c("B01001_001"),geometry = FALSE, year=2019) %>%
  clean_names() %>%
  select(state = name,
         population=estimate) 


ppp_totals_by_state_url <- "https://projects.propublica.org/coronavirus/bailouts/"

ppp_totals_by_state <- ppp_totals_by_state_url %>%
  read_html() %>%
  html_table()

ppp_totals_by_state <- ppp_totals_by_state[[1]] %>%
  clean_names() %>%
  mutate(total_ppp_loans=parse_number(total)) %>%
  select(state,total_ppp_loans) %>%
  inner_join(population_state)
  




```

```{r}
sba_oig_url <- "https://www.sba.gov/document?sortBy=Effective%20Date&search=&documentType=All&program=Pandemic%20Oversight&documentActivity=All&page=1&office=All&relatedOffice=All"

# Read in all html from table, store all tables on page as nested list of dataframes.
sba_oig_url <- sba_oig %>%
  read_html() %>%
  html_text()
  
sba_oig_url
```

```{r}


# Define url of page we want to scrape

naics_url <- "https://www.bls.gov/ces/naics/"

# Read in all html from table, store all tables on page as nested list of dataframes.
naics_industry  <- naics_url %>%
  read_html() %>%
  html_table()

# Just keep the second dataframe in our list, standardize column headers, remove last row

naics_industry <- naics_industry[[2]] %>%
  clean_names() %>%
  slice(-21)

# show the dataframe
naics_industry

# Make a column with URL and xpath ID for each sector, remove the Public Administration sector
naics_industry <- naics_industry %>%
  mutate(sector_url = paste0("https://www.bls.gov/iag/tgs/iag",sector,".htm")) %>%
  mutate(sector_xpath_id =paste0("iag",sector,"emp1")) %>%
  filter(description != "Public Administration")
# Display it
naics_industry

# Create an empty dataframe to hold results
employment_by_sector_all <- tibble()

# For loop, iterating over each row in our naics industry dataframe
for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number)

    # Define url of page to get
    url <- each_row_df$sector_url

    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')

    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table().  The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url %>%
      read_html() %>%
      html_elements(xpath = xpath_employment_table) %>%
      html_table()

    # Grab the dataframe out of the list (it's the first and only element inside the list); clean up the field names with clean_names(); use slice(2) to keep only the second row; use bind_cols() to append the sector code and name to this table; turn jun_2021 column into a proper number, and rename it.  Then select only three columns we need.
    employment_info <- employment_info[[1]] %>%
      clean_names() %>%
      slice(2) %>%
      bind_cols(each_row_df) %>%
      mutate(jun_2021 = parse_number(jun_2021)) %>%
      rename(jun_2021_employees = jun_2021) %>%
      select(sector,description,jun_2021_employees)

    # Bind each individual employment info table to our employment_by_sector_all dataframe
    employment_by_sector_all <- employment_by_sector_all %>%
      bind_rows(employment_info)

}

# Display the completed dataframe
employment_by_sector_all
```

```

## Get Our PDF

We'll be working with the [Maryland Statistical Handbook](https://planning.maryland.gov/MSDC/Documents/md-statistical-handbook.pdf) from the state Department of Planning. It's not tiny (44MB), and you'll want to download it to a place you'll remember (like your Downloads folder).

## Setup Tabula

Start Tabula, then go to http://127.0.0.1:8080/ in your browser. Click the "Browse" button and find the Statistical Handbook file and click "open", and then click the "Import button" in Tabula. This will take a minute or two.

Let's go to page 30 of the PDF, Table 2A, "International Migration for Maryland's Jurisdictions, July 1, 2010 to July 1, 2019". This is _net migration_, meaning it shows the total number of people moving to Maryland and its regions/jurisdictions _minus_ the total number of people leaving the same, divided into 12-month periods from July to July. In Tabula, draw a box around that table's border and click the "Preview & Export Extracted Data" button. It should look pretty clean. Let's export that CSV (it should be called `tabula-md-statistical-handbook.csv` by default) to your Downloads folder.

## Cleaning up the data in R

Let's load it into R, and in doing so we'll skip the first two rows and add our own headers that are cleaner. `read_csv` allows us to do this ([and more](https://readr.tidyverse.org/reference/read_delim.html)):

```{r}
international_migration <- read_csv('tabula-md-statistical-handbook.csv', skip=2, col_names=c("jurisdiction", "july_2011", "july_2012", "july_2013", "july_2014", "july_2015", "july_2016", "july_2017", "july_2018", "july_2019", "total"))
```

Add a column for the type of migration ("international") and populate it:

```{r}
international_migration <- international_migration %>% mutate(type='international')
```

## Answer questions

Q1. Which region and county/city below the state level accounted for the largest percentage of international migration overall?  You'll need to add and populate columns representing percent of total using `mutate`.
A1. Suburban Washington (62.8%). Montgomery County (36.8%).

```{r}
international_migration %>%
  mutate(pct_total=(total/198996)*100) %>%
  select(jurisdiction, pct_total) %>%
  arrange(desc(pct_total))
```

Q2. Write a sentence or two that describes the data you produced in A1. Treat this as if you were trying to convey the most important part.
A2. International migrants to Maryland during the past 10 years were drawn mostly to the suburbs surrounding Washington, D.C., which accounted for more than six of every 10 foreign residents that moved to the state between July 2010 and July 2019.

Q3. Which region & jurisdiction had the biggest percentage change for international migration between July 2018 and July 2017? The formula for percentage change is easy to remember: (New-Old)/Old.
A3. Calvert County's international migrants declined 76 percent (albeit from a small population), and the Southern Maryland region declined 58.5 percent.

```{r}
international_migration %>%
  mutate(pct_change=((july_2018-july_2017)/july_2017)*100) %>%
  select(jurisdiction, pct_change) %>%
  arrange(pct_change)
```

Q4. What's your best guess as to why these declines occurred, and in those area in particular?
A4. One answer is that with the advent of the Trump administration, controls on international migration tightened.

## Back to Tabula

Let's go to page 31 of the PDF, Table 2B, "Domestic Migration for Maryland's Jurisdictions, July 1, 2010 to July 1, 2019". In Tabula, hit the "Clear All Selections" button and then draw a box around that table's border and click the "Preview & Export Extracted Data" button. It should look pretty clean. Let's export that CSV to your Downloads folder (let's rename it to `tabula-md-statistical-handbook-domestic.csv`).

## Cleaning up the data in R

Let's load it into R, and in doing so we'll skip the first two rows and add our own headers that are cleaner:

```{r}
domestic_migration <- read_csv('tabula-md-statistical-handbook-domestic.csv', skip=2, col_names=c("jurisdiction", "july_2011", "july_2012", "july_2013", "july_2014", "july_2015", "july_2016", "july_2017", "july_2018", "july_2019", "total"))
```

Add a column for the type of migration ("domestic") and populate it:

```{r}
domestic_migration <- domestic_migration %>% mutate(type='domestic')
```

## Answer questions
Q5. Which Maryland individual jurisdiction saw the largest net decrease in domestic migration overall?
A5. Baltimore City, with -62,834, with Prince George's County not far behind at -60,167.

```{r}
domestic_migration %>%
  select(jurisdiction, total) %>%
  arrange(total)
```

Q6. How many regions & jurisdictions had net positive migration for July 2017, July 2018 and July 2019?
A6. 14, including the Southern Maryland and Upper Eastern Shore regions.

```{r}
domestic_migration %>%
  filter(july_2017 > 0 & july_2018 > 0 & july_2019 > 0) %>%
  select(jurisdiction, july_2017, july_2018, july_2019)
```

Q7. How would you describe this data? Is there a county or region that stands out, and why?
A7. Frederick and Charles stand out for consistent growth, and Caroline for trending downwards.
