# Automating analysis

Many of the data analyses that you do will be largely one-off efforts -- you're going to do the analysis and write the story and be done. Maybe you'll come back to it in a couple of months or years, but really you're just doing it once.

But what happens when you have a long-running story, where you're going to update it every day, or every week? What changes when you're writing that code?

1. How will this run again without changing anything?
2. What questions do you have that have to be answered each time?
3. What changes when you have to repeat questions to changing data?

The global COVID-19 pandemic is something we're going to be writing about and covering for some time. One element of it is data on vaccinations. That's reported frequently (mostly daily), and we'll be talking about it for some time. So it is an ideal candidate for repeating analysis -- scripting the questions we want to answer every week and doing so in a way that we can just load it without having to change anything.

Let's get some other libraries to our typical tidyverse import (plus lubridate and janitor to help wrangle the data). First, I'm going to add `DT`. It stands for datatables, and it makes your dataframes into html tables that are browsable and searchable. This is a bigger issue for me -- the author who is turning these into html pages -- than you, working in your notebooks. We're also going to add a library called `ggrepel`, which assists in putting tables on dots in charts.

You install them the same way you do anything else -- `install.packages("DT")` and `install.packages("ggrepel")`.

```{r}
library(tidyverse)
library(janitor)
library(lubridate)
library(DT)
library(ggrepel)
```

## Automating downloads and imports

Maryland [publishes data daily on vaccinations on the state Department of Heath website](https://coronavirus.maryland.gov/#Vaccine).

The dashboard is a series of HTML pages placed on the main page via iframe. Like this one: https://state-of-maryland.github.io/VaccineDashboardGraphs/VaccinationDosesDaily.html. But if you view the source on that URL, there's ... no data. It's being pulled in from another URL. If we want to automate getting this data, we'll need to find out where it lives. One big clue is in the composition of that URL: "github.io". Are these files (and maybe the data) already on GitHub? Let's find out. The easiest thing to do is to go to the GitHub user: https://github.com/state-of-maryland. There's a repository there called `VaccineCSVs` that sounds pretty good. Let's check it out.

There are a lot of files here: https://github.com/state-of-maryland/VaccineCSVs, and we want to focus our attention on the ones that are frequently updated. There are a bunch of those, some of which end in .json and .csv.xml, but we want to focus on the ones that end in .csv. Let's use this one: https://raw.githubusercontent.com/state-of-maryland/VaccineCSVs/master/MD_COVID19_TotalVaccinationsCountyFirstandSecondSingleDose.csv

We'll read it into a dataframe like usual and clean up the column names:

```{r}
county_vaccinations <- read_csv("https://raw.githubusercontent.com/state-of-maryland/VaccineCSVs/master/MD_COVID19_TotalVaccinationsCountyFirstandSecondSingleDose.csv") %>%
   clean_names()
View(county_vaccinations)
```

Each row represents vaccination stats for a single county on a single day, including cumulative figures. Let's see if there's anything unusual in the data by counting the number of rows for each county




The table with daily data is located at the bottom of that site, and if you look at the HTML source (always look at the HTML source), you can see that it's an iframe of another page. And not just any page, but a github.io site: https://state-of-maryland.github.io/COVID19_Cases_DashboardBlackBox/CasesBlackboxStats1.html.

That file is updated most weekdays with cumulative figures, and the date isn't included. What we need to do, in words, is this:

1. Read the URL
2. Scrape the table
3. Save the results to a dataframe
4. Add today's date
5. Write the dataframe out to a CSV file

But there's a catch. When you try to use rvest to scrape this HTML table, you won't see the actual numbers:

```{r}
cases_url <- "https://state-of-maryland.github.io/COVID19_Cases_DashboardBlackBox/CasesBlackboxStats1.html"
cases <- cases_url %>%
  read_html() %>%
  html_table()

View(cases[[1]] %>% as_tibble())
```

That's because the data is getting populated on the web page _after_ the page loads in a browser. The good news is that the data has to come from some place, and in this case all the data is in a JSON file. JSON is basically a dictionary object: it has keys and values. A sample JSON object might look like this:

```
{
  "university": "University of Maryland",
  "college": "Merrill College of Journalism",
  "course": "JOUR472/772",
  "course_title": "Data Journalism"
}
```

The keys in this file are to the left of the colon, and the values are to the right. You get to the values by using the keys.

R has several libraries for dealing with JSON data, and we'll use one called `jsonlite`. Let's install it using the usual syntax: `install.packages("jsonlite")`

Now we can load it:

```{r}
library(jsonlite)
```

Next we'll read the JSON file from Maryland and use janitor's `clean_names()` to make the columns easier to deal with:

```{r}
covid_data <- fromJSON("https://state-of-maryland.github.io/COVID19_Cases_DashboardBlackBox/DailyCasesBlackbox.json") %>%
  clean_names()
View(covid_data)
```

Looking at this dataframe, we can see that there are four keys for each object that have been turned into columns: `description`, `cases`, `deaths` and `probable_deaths`. You can see all of the possible values for any of them using the following syntax:

```{r}
covid_data$description
```

We need to clean this dataframe a bit more, to remove those colons from `description` and to make the other columns into numeric data (and remove the commas). We'll use `str_replace` and `str_replace_all` to do that. We'll also remove any rows where `cases`, `deaths` and `probable_deaths` are all NA. We can then use `today()`, a function from lubridate, to add today's date:

```{r}

covid_data <- covid_data %>%
   mutate(description = str_replace(description, ':', '')) %>%
   mutate(cases = as.numeric(str_replace_all(cases,',',''))) %>%
   mutate(deaths = as.numeric(str_replace_all(deaths,',',''))) %>%
   mutate(probable_deaths = as.numeric(str_replace_all(deaths,',',''))) %>%
   filter(!is.na(cases), !is.na(deaths), !is.na(probable_deaths)) %>%
   mutate(date=today())
```

Ok, so we've got a dataframe with _today's_ data. But every time that JSON file gets updated, all of the data gets replaced. So if you want to know what's happening over time, you have have to grab that file every day, process it and add it to your previous data so you can see what has changed. Here's the basic process:

1. Get the new data
2. Clean it and add today's date
3. Add it to previous data
4. Calculate changes
5. Summarize those changes

## The previous data





## Exploring the data

Let's take a look at what we have, using datatables. The formatDate business just makes the date look nicer.

```{r}
datatable(covid_data) %>% formatDate(1, "toLocaleDateString")
```

Let's just look at the most recent week, and that's something that takes on different meaning when we're talking about updating data. We need to make this generic so that every time we pull this up and run it, it's the most recent week at the top. This time, it's very simple.

```{r}
weeklyclaims20 %>% arrange(desc(week_ending_date))
```

## Analysis

Now is when we need to start asking ourselves -- what are the questions that are going to come up week after week. What about how this most current week compares to all weeks going back to 2012? What if we just ranked them? Where does this week rank? For that, we'll create a new column called Rank using mutate and we'll use a function called `min_rank` to rank them. I'm going to save them to a dataframe and use data

```{r}
ranked <- weeklyclaims %>% mutate(Rank = min_rank(desc(initial_claims))) %>% arrange(desc(week_ending_date))

datatable(ranked) %>% formatDate(1, "toLocaleDateString")
```

Let's think about this a little more. What else could we do with this. What are the recurring questions? How about the percent change between this week and last week?

To do that, we need our dates to be next to each other -- side by side. Then we can do new minus old divided by old. To do that, we're going to use a function from `tidyr` called pivot_wider, which will transform our data from one row per week to one row, with the weeks as columns.

```{r}
change <- weeklyclaims20 %>% pivot_wider(names_from = week_ending_date, values_from = initial_claims)

head(change)
```

Now the problem we have is ... which column is the last one, and which one is the previous one? I'll be honest, this isn't easy in R. But the trick is to reverse the order of the columns. Then, your newest one is column 1 and the next newest is 2.  

```{r}
changecalc <- ((rev(change)[1] - rev(change)[2])/rev(change)[2])*100

changecalc
```

So whatever the date, that'll always return the percent change between the most recent date and the previous week.

## Making updating graphics

More than numbers, we are going to want to see this data. We can build this in steps. First, let's just make a big bar chart.

```{r fig.width=9}
ggplot() +
  geom_bar(data=ranked, aes(x=week_ending_date, weight=initial_claims))
```

So that shows us that the trend is going down over time, and that there's some regular spikes around the holidays. Which tells us this data is seasonal, but we knew that going in.

Let's build up some more layers to highlight trends and the most recent spot.

First, we'll slice out a dataframe that's just the most recent data.

```{r}
latest <- ranked %>% slice(1)
```

Now, in ggplot, we can add multiple layers.

The first layer will be all the bars.

The second layer will just be the latest.

Then we'll add a point to the top of that line to really draw attention to it.

Then we'll use ggprepel to label it.

Then I'm going to add a smoothing line. That'll illustrate the trend clearly.

The rest is labeling and adjusting the text to make it look more like a news graphic.

```{r fig.width=9}
ggplot() +
  geom_bar(data=ranked, aes(x=week_ending_date, weight=initial_claims)) +
  geom_bar(data=latest, aes(x=week_ending_date, weight=initial_claims), fill="red") +
  geom_point(data=latest, aes(x=week_ending_date, y=initial_claims)) +
  geom_text_repel(data=latest, aes(x=week_ending_date, y=initial_claims + 150, label="This week")) +
  geom_smooth(data=ranked, aes(x=week_ending_date, y=initial_claims), method=loess, se=FALSE) +
  labs(title="Nebraska jobless claims on the rise", x="Date", y="Claims") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.title = element_text(size = 8),
    plot.subtitle = element_text(size=10),
    panel.grid.minor = element_blank()
    )
```

One thing we are missing? Context. What if we programmatically wrote the chatter for this chart using the percent change calculation we did before?

First, we format the change to look more news graphic like and not with 7 significant digits.

```{r}
changetext <- round(changecalc[[1]], digits=2)
```

Now we're going to use a function called paste to merge some text together. We're going to paste together a sentence fragment, the percent change number and another sentence fragment together to form a sentence. We'll save it as sub, because that's what it's called in ggplot -- a subtitle.

```{r}
sub <- paste("First time unemployment claims rose by ", changetext, " percent over last week", sep="")
```

Here's our sentence:

```{r}
sub
```

Now we can add that to our labels.

```{r fig.width=9}
ggplot() +
  geom_bar(data=ranked, aes(x=week_ending_date, weight=initial_claims)) +
  geom_bar(data=latest, aes(x=week_ending_date, weight=initial_claims), fill="red") +
  geom_point(data=latest, aes(x=week_ending_date, y=initial_claims)) +
  geom_text_repel(data=latest, aes(x=week_ending_date, y=initial_claims + 150, label="This week")) +
  geom_smooth(data=ranked, aes(x=week_ending_date, y=initial_claims), method=loess, se=FALSE) +
  labs(title="Nebraska jobless claims on the rise", subtitle=sub, x="Date", y="Claims") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.title = element_text(size = 8),
    plot.subtitle = element_text(size=10),
    panel.grid.minor = element_blank()
    )
```

This is going to be a story for months, if not years. So repeating this analysis is a must for a reporter covering the economy in Nebraska. We've set ourselves up to do this every week when the data comes out. We just open our notebook, go to Run > Restart R and Run All Chunks and sit back and watch as it does it all again.

Then we go report.

## The State vs the Feds

As I write this, the state hasn't updated their data but the feds have, and the feds have more. The problem? It's a mess. And it's not automatable. So to get more data, you need to go to [the Department of Labor website](https://oui.doleta.gov/unemploy/claims.asp) and fill out the form.

Leave the years, select state, pick yours (I'm taking Nebraska) and select spreadsheet, but know that it's a lie. If you try to load it using read_excel, you'll get an error. Why? Because what is being downloaded is an HTML file. So you can use `rvest` to read and parse it.

However, that isn't so simple.

```{r}
library(rvest)
library(lubridate)
```

First, read the file you downloaded. You'll need to add the `fill=TRUE` to html_table() to solve a problem with some of the data.

```{r, error=TRUE}
claims <- read_html("~/Downloads/r539cy.xls") %>%
  html_nodes("table") %>%
  html_table(fill=TRUE)
```

Similar to previous efforts, we get a list. The first element is a dataframe, so let's get that.

```{r, error=TRUE}
claims <- claims[[1]]
```

Now if you look at this data, you'll see that the header row is empty, and the header names are in the first row. Also, none of this data is formatted correctly. We need to fix that.

To do this, we're going to fix this in steps after we create a new dataframe called `cleanclaims`

1. We'll remove empty columns.
2. We'll rename the columns with what they are, using `janitor` style naming conventions.
3. We'll filter out the old header row.
4. We'll mutate each field to format them correctly. The date columns will use `lubridate`'s `mdy` function. Numbers will use `readr`'s `parse_number` function to solve the comma separator issue.

```{r, error=TRUE}
cleanclaims <- claims %>%
  remove_empty("cols") %>%
  rename("state" = 1, "filed_week_ended"= 2, "initial_claims"=3, "reflecting_week_ended"=4, "continued_claims"=5, "covered_employment"=6, "insured_unemployment_rate"=7) %>%
  filter(state != "State") %>%
  mutate(filed_week_ended = mdy(filed_week_ended), initial_claims=parse_number(initial_claims), reflecting_week_ended=mdy(reflecting_week_ended), continued_claims=parse_number(continued_claims), covered_employment=parse_number(covered_employment), insured_unemployment_rate=parse_number(insured_unemployment_rate))
```

If you open `cleanclaims`, you may notice something:

```{r, error=TRUE}
cleanclaims %>% arrange(desc(filed_week_ended)) %>% head()
```

See it? The latest data isn't in there in my version. [It's in a press release](https://www.dol.gov/sites/dolgov/files/OPA/newsreleases/ui-claims/20200551.pdf).

So how do we add it? We use `add_row` from the tidyverse (specifically the `tibble` library).

```{r, error=TRUE}
updatedcleanclaims <- cleanclaims %>% add_row(state="Nebraska", filed_week_ended=as.Date("2020-03-28"), initial_claims=24572, reflecting_week_ended=as.Date("2020-03-21"))
```

Now we can repeat the analysis from above.

First we rank.

```{r, error=TRUE}
fedranked <- updatedcleanclaims %>% mutate(Rank = min_rank(desc(initial_claims))) %>% arrange(desc(filed_week_ended))
```

We get the latest week.

```{r, error=TRUE}
fedlatest <- fedranked %>% slice(1)
```

The previous week ...

```{r, error=TRUE}
fedprevious <- fedranked %>% slice(2)
```

And for this one, let's grab the pre-crisis number as well as a basis point.

```{r, error=TRUE}
pre <- fedranked %>% filter(filed_week_ended==as.Date("2020-03-14"))
```

Now we can do some percent change math similar to above. First we calculate the current change.

```{r, error=TRUE}
change <- round((((fedlatest$initial_claims - fedprevious$initial_claims)/fedprevious$initial_claims)*100), digits=0)
```

Then the change from the pre-crisis week.

```{r, error=TRUE}
prechange <- round((((fedlatest$initial_claims - pre$initial_claims)/pre$initial_claims)*100), digits=0)
```

And we roll it all into a subhead that we can use in the chart with a little paste-fu.

```{r, error=TRUE}
subhed <- paste("Applications for unemployment jumped ", change, " percent from last week to this week and are up ", prechange, " percent since March 14.", sep="")
```

And we make the graphic.

```{r, error=TRUE, fig.width=9}
ggplot() +
  geom_line(data=fedranked, aes(x=filed_week_ended, y=initial_claims, group=1)) +
  geom_point(data=fedlatest, aes(x=filed_week_ended, y=initial_claims)) +
  geom_text(data=fedlatest, aes(x=filed_week_ended-500, y=initial_claims + 500, label=initial_claims)) +
  geom_point(data=fedprevious, aes(x=filed_week_ended, y=initial_claims)) +
  geom_text(data=fedprevious, aes(x=filed_week_ended-500, y=initial_claims + 500, label=initial_claims)) +
  geom_smooth(data=fedranked, aes(x=filed_week_ended, y=initial_claims), method=loess, se=FALSE) +
  labs(title="Another record for jobless claims in Nebraska", subtitle=subhed, x="Date", y="Claims", caption = "Source: US Dept. of Labor  |  Graphic by Matt Waite") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.title = element_text(size = 8),
    plot.subtitle = element_text(size=10),
    panel.grid.minor = element_blank()
    )
```
